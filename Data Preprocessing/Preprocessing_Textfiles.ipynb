{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_notes_path = \"\" # path to condensed notes file \"condensed_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(textfile_path):\n",
    "    \"\"\" Returns a list of lists. Example:\n",
    "    [['medical', 'oncology', 'progress', 'note', 'dr', 'name', 'zzz', 'yyy'],\n",
    "     ['tumor',\n",
    "      'appears',\n",
    "      'to',\n",
    "      'be',\n",
    "      'having',\n",
    "      'erosion',\n",
    "      'into',\n",
    "      'the',\n",
    "      'cartilage'],\n",
    "     ['right', 'nasal', 'vault', 'has', 'been', 'normal']]\n",
    "     \"\"\"\n",
    "\n",
    "    text_file_to_read = open(textfile_path, 'rb')\n",
    "    all_lines = text_file_to_read.readlines()\n",
    "    all_lines_concatenated = b''.join(all_lines)\n",
    "    all_lines_concatenated = all_lines_concatenated.decode('ascii', errors='ignore')\n",
    "    all_lines_tokenized_by_sentence = nltk.tokenize.sent_tokenize(all_lines_concatenated)\n",
    "    all_lines_tokenized_by_sentence_and_word = []\n",
    "\n",
    "    for sent in all_lines_tokenized_by_sentence:\n",
    "        all_lines_tokenized_by_sentence_and_word.append(gensim.utils.simple_preprocess(sent))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(all_lines_tokenized_by_sentence_and_word):\n",
    "        if len(all_lines_tokenized_by_sentence_and_word[i]) > 512: #Threshold suggested by Samir\n",
    "            del all_lines_tokenized_by_sentence_and_word[i]\n",
    "            i -= 1\n",
    "        i += 1\n",
    "\n",
    "    text_file_to_read.close()\n",
    "    return all_lines_tokenized_by_sentence_and_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_sentence_list(in_folder_path):\n",
    "    directory = os.fsencode(in_folder_path)\n",
    "    complete_sentence_list = []\n",
    "    for i, file in enumerate(os.listdir(directory)):\n",
    "        decoded_file = os.fsdecode(file)\n",
    "        text_file_name = in_folder_path + '/' + decoded_file\n",
    "        curr_text_file_sentence_list = get_sentence_list(text_file_name)\n",
    "        for sent in curr_text_file_sentence_list:\n",
    "            complete_sentence_list.append(sent)\n",
    "    return complete_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_complete_sentence_list(condensed_notes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(e) for e in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open('all_sentences_list', 'wb')\n",
    "pickle.dump(sentences, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}